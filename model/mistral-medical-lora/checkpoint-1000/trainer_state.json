{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9133462724055258,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009133462724055257,
      "grad_norm": 0.3013763427734375,
      "learning_rate": 0.0001,
      "loss": 1.4276,
      "step": 10
    },
    {
      "epoch": 0.018266925448110514,
      "grad_norm": 0.3728012144565582,
      "learning_rate": 0.0002,
      "loss": 1.2323,
      "step": 20
    },
    {
      "epoch": 0.027400388172165772,
      "grad_norm": 0.31142672896385193,
      "learning_rate": 0.00019907749077490775,
      "loss": 1.0811,
      "step": 30
    },
    {
      "epoch": 0.03653385089622103,
      "grad_norm": 0.25764018297195435,
      "learning_rate": 0.00019815498154981552,
      "loss": 1.075,
      "step": 40
    },
    {
      "epoch": 0.04566731362027629,
      "grad_norm": 0.27453529834747314,
      "learning_rate": 0.00019723247232472326,
      "loss": 1.0152,
      "step": 50
    },
    {
      "epoch": 0.054800776344331545,
      "grad_norm": 0.2233482152223587,
      "learning_rate": 0.000196309963099631,
      "loss": 0.9377,
      "step": 60
    },
    {
      "epoch": 0.0639342390683868,
      "grad_norm": 0.24355782568454742,
      "learning_rate": 0.00019538745387453877,
      "loss": 0.9379,
      "step": 70
    },
    {
      "epoch": 0.07306770179244206,
      "grad_norm": 0.22107098996639252,
      "learning_rate": 0.00019446494464944652,
      "loss": 0.9423,
      "step": 80
    },
    {
      "epoch": 0.08220116451649731,
      "grad_norm": 0.2265830636024475,
      "learning_rate": 0.00019354243542435426,
      "loss": 0.998,
      "step": 90
    },
    {
      "epoch": 0.09133462724055258,
      "grad_norm": 0.24885812401771545,
      "learning_rate": 0.000192619926199262,
      "loss": 0.9318,
      "step": 100
    },
    {
      "epoch": 0.10046808996460783,
      "grad_norm": 0.20606379210948944,
      "learning_rate": 0.00019169741697416974,
      "loss": 0.9388,
      "step": 110
    },
    {
      "epoch": 0.10960155268866309,
      "grad_norm": 0.2483375072479248,
      "learning_rate": 0.00019077490774907748,
      "loss": 0.9205,
      "step": 120
    },
    {
      "epoch": 0.11873501541271834,
      "grad_norm": 0.2221204787492752,
      "learning_rate": 0.00018985239852398525,
      "loss": 0.9507,
      "step": 130
    },
    {
      "epoch": 0.1278684781367736,
      "grad_norm": 0.21750620007514954,
      "learning_rate": 0.000188929889298893,
      "loss": 0.8758,
      "step": 140
    },
    {
      "epoch": 0.13700194086082887,
      "grad_norm": 0.22370903193950653,
      "learning_rate": 0.00018800738007380074,
      "loss": 0.8797,
      "step": 150
    },
    {
      "epoch": 0.1461354035848841,
      "grad_norm": 0.20983123779296875,
      "learning_rate": 0.0001870848708487085,
      "loss": 0.9322,
      "step": 160
    },
    {
      "epoch": 0.15526886630893938,
      "grad_norm": 0.19913350045681,
      "learning_rate": 0.00018616236162361625,
      "loss": 0.9371,
      "step": 170
    },
    {
      "epoch": 0.16440232903299462,
      "grad_norm": 0.21175818145275116,
      "learning_rate": 0.000185239852398524,
      "loss": 0.9291,
      "step": 180
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 0.21299564838409424,
      "learning_rate": 0.00018431734317343173,
      "loss": 0.9119,
      "step": 190
    },
    {
      "epoch": 0.18266925448110516,
      "grad_norm": 0.19879214465618134,
      "learning_rate": 0.0001833948339483395,
      "loss": 0.9336,
      "step": 200
    },
    {
      "epoch": 0.1918027172051604,
      "grad_norm": 0.21600374579429626,
      "learning_rate": 0.00018247232472324724,
      "loss": 0.9199,
      "step": 210
    },
    {
      "epoch": 0.20093617992921567,
      "grad_norm": 0.21935969591140747,
      "learning_rate": 0.00018154981549815499,
      "loss": 0.9271,
      "step": 220
    },
    {
      "epoch": 0.2100696426532709,
      "grad_norm": 0.19997148215770721,
      "learning_rate": 0.00018062730627306276,
      "loss": 0.9228,
      "step": 230
    },
    {
      "epoch": 0.21920310537732618,
      "grad_norm": 0.22181983292102814,
      "learning_rate": 0.0001797047970479705,
      "loss": 0.9191,
      "step": 240
    },
    {
      "epoch": 0.22833656810138145,
      "grad_norm": 0.22972598671913147,
      "learning_rate": 0.00017878228782287824,
      "loss": 0.8509,
      "step": 250
    },
    {
      "epoch": 0.2374700308254367,
      "grad_norm": 0.20209188759326935,
      "learning_rate": 0.00017785977859778598,
      "loss": 0.8808,
      "step": 260
    },
    {
      "epoch": 0.24660349354949196,
      "grad_norm": 0.21355898678302765,
      "learning_rate": 0.00017693726937269372,
      "loss": 0.909,
      "step": 270
    },
    {
      "epoch": 0.2557369562735472,
      "grad_norm": 0.21358487010002136,
      "learning_rate": 0.00017601476014760147,
      "loss": 0.8782,
      "step": 280
    },
    {
      "epoch": 0.26487041899760244,
      "grad_norm": 0.20727936923503876,
      "learning_rate": 0.00017509225092250923,
      "loss": 0.9037,
      "step": 290
    },
    {
      "epoch": 0.27400388172165774,
      "grad_norm": 0.22119593620300293,
      "learning_rate": 0.00017416974169741698,
      "loss": 0.904,
      "step": 300
    },
    {
      "epoch": 0.283137344445713,
      "grad_norm": 0.22062358260154724,
      "learning_rate": 0.00017324723247232472,
      "loss": 0.8482,
      "step": 310
    },
    {
      "epoch": 0.2922708071697682,
      "grad_norm": 0.22013501822948456,
      "learning_rate": 0.0001723247232472325,
      "loss": 0.8602,
      "step": 320
    },
    {
      "epoch": 0.3014042698938235,
      "grad_norm": 0.2301793247461319,
      "learning_rate": 0.00017140221402214023,
      "loss": 0.9698,
      "step": 330
    },
    {
      "epoch": 0.31053773261787876,
      "grad_norm": 0.1990135908126831,
      "learning_rate": 0.00017047970479704797,
      "loss": 0.9473,
      "step": 340
    },
    {
      "epoch": 0.319671195341934,
      "grad_norm": 0.20537593960762024,
      "learning_rate": 0.00016955719557195574,
      "loss": 0.9455,
      "step": 350
    },
    {
      "epoch": 0.32880465806598924,
      "grad_norm": 0.20992253720760345,
      "learning_rate": 0.00016863468634686348,
      "loss": 0.8558,
      "step": 360
    },
    {
      "epoch": 0.33793812079004454,
      "grad_norm": 0.19600069522857666,
      "learning_rate": 0.00016771217712177123,
      "loss": 0.8776,
      "step": 370
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 0.21098431944847107,
      "learning_rate": 0.00016678966789667897,
      "loss": 0.8744,
      "step": 380
    },
    {
      "epoch": 0.356205046238155,
      "grad_norm": 0.26497796177864075,
      "learning_rate": 0.00016586715867158674,
      "loss": 0.8863,
      "step": 390
    },
    {
      "epoch": 0.3653385089622103,
      "grad_norm": 0.21664339303970337,
      "learning_rate": 0.00016494464944649448,
      "loss": 0.8327,
      "step": 400
    },
    {
      "epoch": 0.37447197168626556,
      "grad_norm": 0.19974668323993683,
      "learning_rate": 0.00016402214022140222,
      "loss": 0.8649,
      "step": 410
    },
    {
      "epoch": 0.3836054344103208,
      "grad_norm": 0.20814305543899536,
      "learning_rate": 0.00016309963099630996,
      "loss": 0.8838,
      "step": 420
    },
    {
      "epoch": 0.3927388971343761,
      "grad_norm": 0.2275354266166687,
      "learning_rate": 0.0001621771217712177,
      "loss": 0.8381,
      "step": 430
    },
    {
      "epoch": 0.40187235985843134,
      "grad_norm": 0.19296669960021973,
      "learning_rate": 0.00016125461254612547,
      "loss": 0.8242,
      "step": 440
    },
    {
      "epoch": 0.4110058225824866,
      "grad_norm": 0.1949186623096466,
      "learning_rate": 0.00016033210332103322,
      "loss": 0.8811,
      "step": 450
    },
    {
      "epoch": 0.4201392853065418,
      "grad_norm": 0.22817623615264893,
      "learning_rate": 0.00015940959409594096,
      "loss": 0.8644,
      "step": 460
    },
    {
      "epoch": 0.4292727480305971,
      "grad_norm": 0.19387534260749817,
      "learning_rate": 0.0001584870848708487,
      "loss": 0.8967,
      "step": 470
    },
    {
      "epoch": 0.43840621075465236,
      "grad_norm": 0.22086107730865479,
      "learning_rate": 0.00015756457564575647,
      "loss": 0.8183,
      "step": 480
    },
    {
      "epoch": 0.4475396734787076,
      "grad_norm": 0.22450858354568481,
      "learning_rate": 0.0001566420664206642,
      "loss": 0.9094,
      "step": 490
    },
    {
      "epoch": 0.4566731362027629,
      "grad_norm": 0.2014199048280716,
      "learning_rate": 0.00015571955719557195,
      "loss": 0.866,
      "step": 500
    },
    {
      "epoch": 0.46580659892681814,
      "grad_norm": 0.212111234664917,
      "learning_rate": 0.00015479704797047972,
      "loss": 0.8452,
      "step": 510
    },
    {
      "epoch": 0.4749400616508734,
      "grad_norm": 0.2563711404800415,
      "learning_rate": 0.00015387453874538746,
      "loss": 0.904,
      "step": 520
    },
    {
      "epoch": 0.4840735243749286,
      "grad_norm": 0.20719636976718903,
      "learning_rate": 0.0001529520295202952,
      "loss": 0.867,
      "step": 530
    },
    {
      "epoch": 0.4932069870989839,
      "grad_norm": 0.2520475685596466,
      "learning_rate": 0.00015202952029520298,
      "loss": 0.8147,
      "step": 540
    },
    {
      "epoch": 0.5023404498230392,
      "grad_norm": 0.19871675968170166,
      "learning_rate": 0.00015110701107011072,
      "loss": 0.8581,
      "step": 550
    },
    {
      "epoch": 0.5114739125470944,
      "grad_norm": 0.19907104969024658,
      "learning_rate": 0.00015018450184501846,
      "loss": 0.8769,
      "step": 560
    },
    {
      "epoch": 0.5206073752711496,
      "grad_norm": 0.20210492610931396,
      "learning_rate": 0.00014926199261992623,
      "loss": 0.8659,
      "step": 570
    },
    {
      "epoch": 0.5297408379952049,
      "grad_norm": 0.19819916784763336,
      "learning_rate": 0.00014833948339483394,
      "loss": 0.8943,
      "step": 580
    },
    {
      "epoch": 0.5388743007192602,
      "grad_norm": 0.21600696444511414,
      "learning_rate": 0.00014741697416974169,
      "loss": 0.8879,
      "step": 590
    },
    {
      "epoch": 0.5480077634433155,
      "grad_norm": 0.20702607929706573,
      "learning_rate": 0.00014649446494464946,
      "loss": 0.8624,
      "step": 600
    },
    {
      "epoch": 0.5571412261673707,
      "grad_norm": 0.20838657021522522,
      "learning_rate": 0.0001455719557195572,
      "loss": 0.868,
      "step": 610
    },
    {
      "epoch": 0.566274688891426,
      "grad_norm": 0.22809827327728271,
      "learning_rate": 0.00014464944649446494,
      "loss": 0.8863,
      "step": 620
    },
    {
      "epoch": 0.5754081516154812,
      "grad_norm": 0.21148039400577545,
      "learning_rate": 0.0001437269372693727,
      "loss": 0.8487,
      "step": 630
    },
    {
      "epoch": 0.5845416143395364,
      "grad_norm": 0.23190122842788696,
      "learning_rate": 0.00014280442804428045,
      "loss": 0.8621,
      "step": 640
    },
    {
      "epoch": 0.5936750770635917,
      "grad_norm": 0.22668448090553284,
      "learning_rate": 0.0001418819188191882,
      "loss": 0.8573,
      "step": 650
    },
    {
      "epoch": 0.602808539787647,
      "grad_norm": 0.198675736784935,
      "learning_rate": 0.00014095940959409593,
      "loss": 0.8453,
      "step": 660
    },
    {
      "epoch": 0.6119420025117023,
      "grad_norm": 0.242654487490654,
      "learning_rate": 0.0001400369003690037,
      "loss": 0.8463,
      "step": 670
    },
    {
      "epoch": 0.6210754652357575,
      "grad_norm": 0.19826917350292206,
      "learning_rate": 0.00013911439114391145,
      "loss": 0.8933,
      "step": 680
    },
    {
      "epoch": 0.6302089279598128,
      "grad_norm": 0.20580920577049255,
      "learning_rate": 0.0001381918819188192,
      "loss": 0.8397,
      "step": 690
    },
    {
      "epoch": 0.639342390683868,
      "grad_norm": 0.21150745451450348,
      "learning_rate": 0.00013726937269372696,
      "loss": 0.857,
      "step": 700
    },
    {
      "epoch": 0.6484758534079232,
      "grad_norm": 0.230162113904953,
      "learning_rate": 0.0001363468634686347,
      "loss": 0.8491,
      "step": 710
    },
    {
      "epoch": 0.6576093161319785,
      "grad_norm": 0.21637894213199615,
      "learning_rate": 0.00013542435424354244,
      "loss": 0.8526,
      "step": 720
    },
    {
      "epoch": 0.6667427788560338,
      "grad_norm": 0.18879272043704987,
      "learning_rate": 0.0001345018450184502,
      "loss": 0.8828,
      "step": 730
    },
    {
      "epoch": 0.6758762415800891,
      "grad_norm": 0.19696685671806335,
      "learning_rate": 0.00013357933579335793,
      "loss": 0.8636,
      "step": 740
    },
    {
      "epoch": 0.6850097043041443,
      "grad_norm": 0.20826105773448944,
      "learning_rate": 0.00013265682656826567,
      "loss": 0.8739,
      "step": 750
    },
    {
      "epoch": 0.6941431670281996,
      "grad_norm": 0.21626412868499756,
      "learning_rate": 0.00013173431734317344,
      "loss": 0.8809,
      "step": 760
    },
    {
      "epoch": 0.7032766297522548,
      "grad_norm": 0.19666989147663116,
      "learning_rate": 0.00013081180811808118,
      "loss": 0.857,
      "step": 770
    },
    {
      "epoch": 0.71241009247631,
      "grad_norm": 0.22540223598480225,
      "learning_rate": 0.00012988929889298892,
      "loss": 0.8665,
      "step": 780
    },
    {
      "epoch": 0.7215435552003653,
      "grad_norm": 0.21336333453655243,
      "learning_rate": 0.0001289667896678967,
      "loss": 0.7937,
      "step": 790
    },
    {
      "epoch": 0.7306770179244206,
      "grad_norm": 0.22190077602863312,
      "learning_rate": 0.00012804428044280443,
      "loss": 0.8699,
      "step": 800
    },
    {
      "epoch": 0.7398104806484759,
      "grad_norm": 0.25819602608680725,
      "learning_rate": 0.00012712177121771217,
      "loss": 0.8504,
      "step": 810
    },
    {
      "epoch": 0.7489439433725311,
      "grad_norm": 0.22666247189044952,
      "learning_rate": 0.00012619926199261994,
      "loss": 0.851,
      "step": 820
    },
    {
      "epoch": 0.7580774060965864,
      "grad_norm": 0.232225701212883,
      "learning_rate": 0.00012527675276752769,
      "loss": 0.8428,
      "step": 830
    },
    {
      "epoch": 0.7672108688206416,
      "grad_norm": 0.20489858090877533,
      "learning_rate": 0.00012435424354243543,
      "loss": 0.8694,
      "step": 840
    },
    {
      "epoch": 0.7763443315446968,
      "grad_norm": 0.19806212186813354,
      "learning_rate": 0.0001234317343173432,
      "loss": 0.869,
      "step": 850
    },
    {
      "epoch": 0.7854777942687522,
      "grad_norm": 0.21383069455623627,
      "learning_rate": 0.00012250922509225094,
      "loss": 0.8474,
      "step": 860
    },
    {
      "epoch": 0.7946112569928074,
      "grad_norm": 0.23688280582427979,
      "learning_rate": 0.00012158671586715868,
      "loss": 0.8821,
      "step": 870
    },
    {
      "epoch": 0.8037447197168627,
      "grad_norm": 0.21636748313903809,
      "learning_rate": 0.00012066420664206644,
      "loss": 0.8141,
      "step": 880
    },
    {
      "epoch": 0.8128781824409179,
      "grad_norm": 0.22281038761138916,
      "learning_rate": 0.00011974169741697419,
      "loss": 0.8327,
      "step": 890
    },
    {
      "epoch": 0.8220116451649732,
      "grad_norm": 0.20660780370235443,
      "learning_rate": 0.00011881918819188192,
      "loss": 0.8281,
      "step": 900
    },
    {
      "epoch": 0.8311451078890284,
      "grad_norm": 0.20863991975784302,
      "learning_rate": 0.00011789667896678966,
      "loss": 0.793,
      "step": 910
    },
    {
      "epoch": 0.8402785706130836,
      "grad_norm": 0.1976911425590515,
      "learning_rate": 0.00011697416974169742,
      "loss": 0.8423,
      "step": 920
    },
    {
      "epoch": 0.849412033337139,
      "grad_norm": 0.21840941905975342,
      "learning_rate": 0.00011605166051660516,
      "loss": 0.8118,
      "step": 930
    },
    {
      "epoch": 0.8585454960611942,
      "grad_norm": 0.21552985906600952,
      "learning_rate": 0.00011512915129151292,
      "loss": 0.791,
      "step": 940
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 0.23272547125816345,
      "learning_rate": 0.00011420664206642067,
      "loss": 0.8344,
      "step": 950
    },
    {
      "epoch": 0.8768124215093047,
      "grad_norm": 0.2061493843793869,
      "learning_rate": 0.00011328413284132841,
      "loss": 0.8195,
      "step": 960
    },
    {
      "epoch": 0.88594588423336,
      "grad_norm": 0.25896310806274414,
      "learning_rate": 0.00011236162361623617,
      "loss": 0.9056,
      "step": 970
    },
    {
      "epoch": 0.8950793469574152,
      "grad_norm": 0.207029327750206,
      "learning_rate": 0.00011143911439114391,
      "loss": 0.8406,
      "step": 980
    },
    {
      "epoch": 0.9042128096814704,
      "grad_norm": 0.25719401240348816,
      "learning_rate": 0.00011051660516605167,
      "loss": 0.8023,
      "step": 990
    },
    {
      "epoch": 0.9133462724055258,
      "grad_norm": 0.21250078082084656,
      "learning_rate": 0.00010959409594095942,
      "loss": 0.9004,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 2188,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "total_flos": 2.331676500863877e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
