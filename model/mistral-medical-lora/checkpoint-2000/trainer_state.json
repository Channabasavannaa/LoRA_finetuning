{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.8266925448110514,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009133462724055257,
      "grad_norm": 0.3013763427734375,
      "learning_rate": 0.0001,
      "loss": 1.4276,
      "step": 10
    },
    {
      "epoch": 0.018266925448110514,
      "grad_norm": 0.3728012144565582,
      "learning_rate": 0.0002,
      "loss": 1.2323,
      "step": 20
    },
    {
      "epoch": 0.027400388172165772,
      "grad_norm": 0.31142672896385193,
      "learning_rate": 0.00019907749077490775,
      "loss": 1.0811,
      "step": 30
    },
    {
      "epoch": 0.03653385089622103,
      "grad_norm": 0.25764018297195435,
      "learning_rate": 0.00019815498154981552,
      "loss": 1.075,
      "step": 40
    },
    {
      "epoch": 0.04566731362027629,
      "grad_norm": 0.27453529834747314,
      "learning_rate": 0.00019723247232472326,
      "loss": 1.0152,
      "step": 50
    },
    {
      "epoch": 0.054800776344331545,
      "grad_norm": 0.2233482152223587,
      "learning_rate": 0.000196309963099631,
      "loss": 0.9377,
      "step": 60
    },
    {
      "epoch": 0.0639342390683868,
      "grad_norm": 0.24355782568454742,
      "learning_rate": 0.00019538745387453877,
      "loss": 0.9379,
      "step": 70
    },
    {
      "epoch": 0.07306770179244206,
      "grad_norm": 0.22107098996639252,
      "learning_rate": 0.00019446494464944652,
      "loss": 0.9423,
      "step": 80
    },
    {
      "epoch": 0.08220116451649731,
      "grad_norm": 0.2265830636024475,
      "learning_rate": 0.00019354243542435426,
      "loss": 0.998,
      "step": 90
    },
    {
      "epoch": 0.09133462724055258,
      "grad_norm": 0.24885812401771545,
      "learning_rate": 0.000192619926199262,
      "loss": 0.9318,
      "step": 100
    },
    {
      "epoch": 0.10046808996460783,
      "grad_norm": 0.20606379210948944,
      "learning_rate": 0.00019169741697416974,
      "loss": 0.9388,
      "step": 110
    },
    {
      "epoch": 0.10960155268866309,
      "grad_norm": 0.2483375072479248,
      "learning_rate": 0.00019077490774907748,
      "loss": 0.9205,
      "step": 120
    },
    {
      "epoch": 0.11873501541271834,
      "grad_norm": 0.2221204787492752,
      "learning_rate": 0.00018985239852398525,
      "loss": 0.9507,
      "step": 130
    },
    {
      "epoch": 0.1278684781367736,
      "grad_norm": 0.21750620007514954,
      "learning_rate": 0.000188929889298893,
      "loss": 0.8758,
      "step": 140
    },
    {
      "epoch": 0.13700194086082887,
      "grad_norm": 0.22370903193950653,
      "learning_rate": 0.00018800738007380074,
      "loss": 0.8797,
      "step": 150
    },
    {
      "epoch": 0.1461354035848841,
      "grad_norm": 0.20983123779296875,
      "learning_rate": 0.0001870848708487085,
      "loss": 0.9322,
      "step": 160
    },
    {
      "epoch": 0.15526886630893938,
      "grad_norm": 0.19913350045681,
      "learning_rate": 0.00018616236162361625,
      "loss": 0.9371,
      "step": 170
    },
    {
      "epoch": 0.16440232903299462,
      "grad_norm": 0.21175818145275116,
      "learning_rate": 0.000185239852398524,
      "loss": 0.9291,
      "step": 180
    },
    {
      "epoch": 0.1735357917570499,
      "grad_norm": 0.21299564838409424,
      "learning_rate": 0.00018431734317343173,
      "loss": 0.9119,
      "step": 190
    },
    {
      "epoch": 0.18266925448110516,
      "grad_norm": 0.19879214465618134,
      "learning_rate": 0.0001833948339483395,
      "loss": 0.9336,
      "step": 200
    },
    {
      "epoch": 0.1918027172051604,
      "grad_norm": 0.21600374579429626,
      "learning_rate": 0.00018247232472324724,
      "loss": 0.9199,
      "step": 210
    },
    {
      "epoch": 0.20093617992921567,
      "grad_norm": 0.21935969591140747,
      "learning_rate": 0.00018154981549815499,
      "loss": 0.9271,
      "step": 220
    },
    {
      "epoch": 0.2100696426532709,
      "grad_norm": 0.19997148215770721,
      "learning_rate": 0.00018062730627306276,
      "loss": 0.9228,
      "step": 230
    },
    {
      "epoch": 0.21920310537732618,
      "grad_norm": 0.22181983292102814,
      "learning_rate": 0.0001797047970479705,
      "loss": 0.9191,
      "step": 240
    },
    {
      "epoch": 0.22833656810138145,
      "grad_norm": 0.22972598671913147,
      "learning_rate": 0.00017878228782287824,
      "loss": 0.8509,
      "step": 250
    },
    {
      "epoch": 0.2374700308254367,
      "grad_norm": 0.20209188759326935,
      "learning_rate": 0.00017785977859778598,
      "loss": 0.8808,
      "step": 260
    },
    {
      "epoch": 0.24660349354949196,
      "grad_norm": 0.21355898678302765,
      "learning_rate": 0.00017693726937269372,
      "loss": 0.909,
      "step": 270
    },
    {
      "epoch": 0.2557369562735472,
      "grad_norm": 0.21358487010002136,
      "learning_rate": 0.00017601476014760147,
      "loss": 0.8782,
      "step": 280
    },
    {
      "epoch": 0.26487041899760244,
      "grad_norm": 0.20727936923503876,
      "learning_rate": 0.00017509225092250923,
      "loss": 0.9037,
      "step": 290
    },
    {
      "epoch": 0.27400388172165774,
      "grad_norm": 0.22119593620300293,
      "learning_rate": 0.00017416974169741698,
      "loss": 0.904,
      "step": 300
    },
    {
      "epoch": 0.283137344445713,
      "grad_norm": 0.22062358260154724,
      "learning_rate": 0.00017324723247232472,
      "loss": 0.8482,
      "step": 310
    },
    {
      "epoch": 0.2922708071697682,
      "grad_norm": 0.22013501822948456,
      "learning_rate": 0.0001723247232472325,
      "loss": 0.8602,
      "step": 320
    },
    {
      "epoch": 0.3014042698938235,
      "grad_norm": 0.2301793247461319,
      "learning_rate": 0.00017140221402214023,
      "loss": 0.9698,
      "step": 330
    },
    {
      "epoch": 0.31053773261787876,
      "grad_norm": 0.1990135908126831,
      "learning_rate": 0.00017047970479704797,
      "loss": 0.9473,
      "step": 340
    },
    {
      "epoch": 0.319671195341934,
      "grad_norm": 0.20537593960762024,
      "learning_rate": 0.00016955719557195574,
      "loss": 0.9455,
      "step": 350
    },
    {
      "epoch": 0.32880465806598924,
      "grad_norm": 0.20992253720760345,
      "learning_rate": 0.00016863468634686348,
      "loss": 0.8558,
      "step": 360
    },
    {
      "epoch": 0.33793812079004454,
      "grad_norm": 0.19600069522857666,
      "learning_rate": 0.00016771217712177123,
      "loss": 0.8776,
      "step": 370
    },
    {
      "epoch": 0.3470715835140998,
      "grad_norm": 0.21098431944847107,
      "learning_rate": 0.00016678966789667897,
      "loss": 0.8744,
      "step": 380
    },
    {
      "epoch": 0.356205046238155,
      "grad_norm": 0.26497796177864075,
      "learning_rate": 0.00016586715867158674,
      "loss": 0.8863,
      "step": 390
    },
    {
      "epoch": 0.3653385089622103,
      "grad_norm": 0.21664339303970337,
      "learning_rate": 0.00016494464944649448,
      "loss": 0.8327,
      "step": 400
    },
    {
      "epoch": 0.37447197168626556,
      "grad_norm": 0.19974668323993683,
      "learning_rate": 0.00016402214022140222,
      "loss": 0.8649,
      "step": 410
    },
    {
      "epoch": 0.3836054344103208,
      "grad_norm": 0.20814305543899536,
      "learning_rate": 0.00016309963099630996,
      "loss": 0.8838,
      "step": 420
    },
    {
      "epoch": 0.3927388971343761,
      "grad_norm": 0.2275354266166687,
      "learning_rate": 0.0001621771217712177,
      "loss": 0.8381,
      "step": 430
    },
    {
      "epoch": 0.40187235985843134,
      "grad_norm": 0.19296669960021973,
      "learning_rate": 0.00016125461254612547,
      "loss": 0.8242,
      "step": 440
    },
    {
      "epoch": 0.4110058225824866,
      "grad_norm": 0.1949186623096466,
      "learning_rate": 0.00016033210332103322,
      "loss": 0.8811,
      "step": 450
    },
    {
      "epoch": 0.4201392853065418,
      "grad_norm": 0.22817623615264893,
      "learning_rate": 0.00015940959409594096,
      "loss": 0.8644,
      "step": 460
    },
    {
      "epoch": 0.4292727480305971,
      "grad_norm": 0.19387534260749817,
      "learning_rate": 0.0001584870848708487,
      "loss": 0.8967,
      "step": 470
    },
    {
      "epoch": 0.43840621075465236,
      "grad_norm": 0.22086107730865479,
      "learning_rate": 0.00015756457564575647,
      "loss": 0.8183,
      "step": 480
    },
    {
      "epoch": 0.4475396734787076,
      "grad_norm": 0.22450858354568481,
      "learning_rate": 0.0001566420664206642,
      "loss": 0.9094,
      "step": 490
    },
    {
      "epoch": 0.4566731362027629,
      "grad_norm": 0.2014199048280716,
      "learning_rate": 0.00015571955719557195,
      "loss": 0.866,
      "step": 500
    },
    {
      "epoch": 0.46580659892681814,
      "grad_norm": 0.212111234664917,
      "learning_rate": 0.00015479704797047972,
      "loss": 0.8452,
      "step": 510
    },
    {
      "epoch": 0.4749400616508734,
      "grad_norm": 0.2563711404800415,
      "learning_rate": 0.00015387453874538746,
      "loss": 0.904,
      "step": 520
    },
    {
      "epoch": 0.4840735243749286,
      "grad_norm": 0.20719636976718903,
      "learning_rate": 0.0001529520295202952,
      "loss": 0.867,
      "step": 530
    },
    {
      "epoch": 0.4932069870989839,
      "grad_norm": 0.2520475685596466,
      "learning_rate": 0.00015202952029520298,
      "loss": 0.8147,
      "step": 540
    },
    {
      "epoch": 0.5023404498230392,
      "grad_norm": 0.19871675968170166,
      "learning_rate": 0.00015110701107011072,
      "loss": 0.8581,
      "step": 550
    },
    {
      "epoch": 0.5114739125470944,
      "grad_norm": 0.19907104969024658,
      "learning_rate": 0.00015018450184501846,
      "loss": 0.8769,
      "step": 560
    },
    {
      "epoch": 0.5206073752711496,
      "grad_norm": 0.20210492610931396,
      "learning_rate": 0.00014926199261992623,
      "loss": 0.8659,
      "step": 570
    },
    {
      "epoch": 0.5297408379952049,
      "grad_norm": 0.19819916784763336,
      "learning_rate": 0.00014833948339483394,
      "loss": 0.8943,
      "step": 580
    },
    {
      "epoch": 0.5388743007192602,
      "grad_norm": 0.21600696444511414,
      "learning_rate": 0.00014741697416974169,
      "loss": 0.8879,
      "step": 590
    },
    {
      "epoch": 0.5480077634433155,
      "grad_norm": 0.20702607929706573,
      "learning_rate": 0.00014649446494464946,
      "loss": 0.8624,
      "step": 600
    },
    {
      "epoch": 0.5571412261673707,
      "grad_norm": 0.20838657021522522,
      "learning_rate": 0.0001455719557195572,
      "loss": 0.868,
      "step": 610
    },
    {
      "epoch": 0.566274688891426,
      "grad_norm": 0.22809827327728271,
      "learning_rate": 0.00014464944649446494,
      "loss": 0.8863,
      "step": 620
    },
    {
      "epoch": 0.5754081516154812,
      "grad_norm": 0.21148039400577545,
      "learning_rate": 0.0001437269372693727,
      "loss": 0.8487,
      "step": 630
    },
    {
      "epoch": 0.5845416143395364,
      "grad_norm": 0.23190122842788696,
      "learning_rate": 0.00014280442804428045,
      "loss": 0.8621,
      "step": 640
    },
    {
      "epoch": 0.5936750770635917,
      "grad_norm": 0.22668448090553284,
      "learning_rate": 0.0001418819188191882,
      "loss": 0.8573,
      "step": 650
    },
    {
      "epoch": 0.602808539787647,
      "grad_norm": 0.198675736784935,
      "learning_rate": 0.00014095940959409593,
      "loss": 0.8453,
      "step": 660
    },
    {
      "epoch": 0.6119420025117023,
      "grad_norm": 0.242654487490654,
      "learning_rate": 0.0001400369003690037,
      "loss": 0.8463,
      "step": 670
    },
    {
      "epoch": 0.6210754652357575,
      "grad_norm": 0.19826917350292206,
      "learning_rate": 0.00013911439114391145,
      "loss": 0.8933,
      "step": 680
    },
    {
      "epoch": 0.6302089279598128,
      "grad_norm": 0.20580920577049255,
      "learning_rate": 0.0001381918819188192,
      "loss": 0.8397,
      "step": 690
    },
    {
      "epoch": 0.639342390683868,
      "grad_norm": 0.21150745451450348,
      "learning_rate": 0.00013726937269372696,
      "loss": 0.857,
      "step": 700
    },
    {
      "epoch": 0.6484758534079232,
      "grad_norm": 0.230162113904953,
      "learning_rate": 0.0001363468634686347,
      "loss": 0.8491,
      "step": 710
    },
    {
      "epoch": 0.6576093161319785,
      "grad_norm": 0.21637894213199615,
      "learning_rate": 0.00013542435424354244,
      "loss": 0.8526,
      "step": 720
    },
    {
      "epoch": 0.6667427788560338,
      "grad_norm": 0.18879272043704987,
      "learning_rate": 0.0001345018450184502,
      "loss": 0.8828,
      "step": 730
    },
    {
      "epoch": 0.6758762415800891,
      "grad_norm": 0.19696685671806335,
      "learning_rate": 0.00013357933579335793,
      "loss": 0.8636,
      "step": 740
    },
    {
      "epoch": 0.6850097043041443,
      "grad_norm": 0.20826105773448944,
      "learning_rate": 0.00013265682656826567,
      "loss": 0.8739,
      "step": 750
    },
    {
      "epoch": 0.6941431670281996,
      "grad_norm": 0.21626412868499756,
      "learning_rate": 0.00013173431734317344,
      "loss": 0.8809,
      "step": 760
    },
    {
      "epoch": 0.7032766297522548,
      "grad_norm": 0.19666989147663116,
      "learning_rate": 0.00013081180811808118,
      "loss": 0.857,
      "step": 770
    },
    {
      "epoch": 0.71241009247631,
      "grad_norm": 0.22540223598480225,
      "learning_rate": 0.00012988929889298892,
      "loss": 0.8665,
      "step": 780
    },
    {
      "epoch": 0.7215435552003653,
      "grad_norm": 0.21336333453655243,
      "learning_rate": 0.0001289667896678967,
      "loss": 0.7937,
      "step": 790
    },
    {
      "epoch": 0.7306770179244206,
      "grad_norm": 0.22190077602863312,
      "learning_rate": 0.00012804428044280443,
      "loss": 0.8699,
      "step": 800
    },
    {
      "epoch": 0.7398104806484759,
      "grad_norm": 0.25819602608680725,
      "learning_rate": 0.00012712177121771217,
      "loss": 0.8504,
      "step": 810
    },
    {
      "epoch": 0.7489439433725311,
      "grad_norm": 0.22666247189044952,
      "learning_rate": 0.00012619926199261994,
      "loss": 0.851,
      "step": 820
    },
    {
      "epoch": 0.7580774060965864,
      "grad_norm": 0.232225701212883,
      "learning_rate": 0.00012527675276752769,
      "loss": 0.8428,
      "step": 830
    },
    {
      "epoch": 0.7672108688206416,
      "grad_norm": 0.20489858090877533,
      "learning_rate": 0.00012435424354243543,
      "loss": 0.8694,
      "step": 840
    },
    {
      "epoch": 0.7763443315446968,
      "grad_norm": 0.19806212186813354,
      "learning_rate": 0.0001234317343173432,
      "loss": 0.869,
      "step": 850
    },
    {
      "epoch": 0.7854777942687522,
      "grad_norm": 0.21383069455623627,
      "learning_rate": 0.00012250922509225094,
      "loss": 0.8474,
      "step": 860
    },
    {
      "epoch": 0.7946112569928074,
      "grad_norm": 0.23688280582427979,
      "learning_rate": 0.00012158671586715868,
      "loss": 0.8821,
      "step": 870
    },
    {
      "epoch": 0.8037447197168627,
      "grad_norm": 0.21636748313903809,
      "learning_rate": 0.00012066420664206644,
      "loss": 0.8141,
      "step": 880
    },
    {
      "epoch": 0.8128781824409179,
      "grad_norm": 0.22281038761138916,
      "learning_rate": 0.00011974169741697419,
      "loss": 0.8327,
      "step": 890
    },
    {
      "epoch": 0.8220116451649732,
      "grad_norm": 0.20660780370235443,
      "learning_rate": 0.00011881918819188192,
      "loss": 0.8281,
      "step": 900
    },
    {
      "epoch": 0.8311451078890284,
      "grad_norm": 0.20863991975784302,
      "learning_rate": 0.00011789667896678966,
      "loss": 0.793,
      "step": 910
    },
    {
      "epoch": 0.8402785706130836,
      "grad_norm": 0.1976911425590515,
      "learning_rate": 0.00011697416974169742,
      "loss": 0.8423,
      "step": 920
    },
    {
      "epoch": 0.849412033337139,
      "grad_norm": 0.21840941905975342,
      "learning_rate": 0.00011605166051660516,
      "loss": 0.8118,
      "step": 930
    },
    {
      "epoch": 0.8585454960611942,
      "grad_norm": 0.21552985906600952,
      "learning_rate": 0.00011512915129151292,
      "loss": 0.791,
      "step": 940
    },
    {
      "epoch": 0.8676789587852495,
      "grad_norm": 0.23272547125816345,
      "learning_rate": 0.00011420664206642067,
      "loss": 0.8344,
      "step": 950
    },
    {
      "epoch": 0.8768124215093047,
      "grad_norm": 0.2061493843793869,
      "learning_rate": 0.00011328413284132841,
      "loss": 0.8195,
      "step": 960
    },
    {
      "epoch": 0.88594588423336,
      "grad_norm": 0.25896310806274414,
      "learning_rate": 0.00011236162361623617,
      "loss": 0.9056,
      "step": 970
    },
    {
      "epoch": 0.8950793469574152,
      "grad_norm": 0.207029327750206,
      "learning_rate": 0.00011143911439114391,
      "loss": 0.8406,
      "step": 980
    },
    {
      "epoch": 0.9042128096814704,
      "grad_norm": 0.25719401240348816,
      "learning_rate": 0.00011051660516605167,
      "loss": 0.8023,
      "step": 990
    },
    {
      "epoch": 0.9133462724055258,
      "grad_norm": 0.21250078082084656,
      "learning_rate": 0.00010959409594095942,
      "loss": 0.9004,
      "step": 1000
    },
    {
      "epoch": 0.922479735129581,
      "grad_norm": 0.19986563920974731,
      "learning_rate": 0.00010867158671586716,
      "loss": 0.8242,
      "step": 1010
    },
    {
      "epoch": 0.9316131978536363,
      "grad_norm": 0.21329014003276825,
      "learning_rate": 0.00010774907749077492,
      "loss": 0.8451,
      "step": 1020
    },
    {
      "epoch": 0.9407466605776915,
      "grad_norm": 0.23205693066120148,
      "learning_rate": 0.00010682656826568268,
      "loss": 0.8965,
      "step": 1030
    },
    {
      "epoch": 0.9498801233017468,
      "grad_norm": 0.2207806259393692,
      "learning_rate": 0.00010590405904059042,
      "loss": 0.8164,
      "step": 1040
    },
    {
      "epoch": 0.959013586025802,
      "grad_norm": 0.23433133959770203,
      "learning_rate": 0.00010498154981549817,
      "loss": 0.8915,
      "step": 1050
    },
    {
      "epoch": 0.9681470487498572,
      "grad_norm": 0.22942766547203064,
      "learning_rate": 0.0001040590405904059,
      "loss": 0.8779,
      "step": 1060
    },
    {
      "epoch": 0.9772805114739126,
      "grad_norm": 0.24961969256401062,
      "learning_rate": 0.00010313653136531364,
      "loss": 0.8095,
      "step": 1070
    },
    {
      "epoch": 0.9864139741979678,
      "grad_norm": 0.22195316851139069,
      "learning_rate": 0.0001022140221402214,
      "loss": 0.8086,
      "step": 1080
    },
    {
      "epoch": 0.9955474369220231,
      "grad_norm": 0.21490196883678436,
      "learning_rate": 0.00010129151291512916,
      "loss": 0.8603,
      "step": 1090
    },
    {
      "epoch": 1.0046808996460783,
      "grad_norm": 0.2065921425819397,
      "learning_rate": 0.0001003690036900369,
      "loss": 0.7997,
      "step": 1100
    },
    {
      "epoch": 1.0138143623701337,
      "grad_norm": 0.20200695097446442,
      "learning_rate": 9.944649446494465e-05,
      "loss": 0.7874,
      "step": 1110
    },
    {
      "epoch": 1.0229478250941888,
      "grad_norm": 0.26544973254203796,
      "learning_rate": 9.85239852398524e-05,
      "loss": 0.7746,
      "step": 1120
    },
    {
      "epoch": 1.0320812878182442,
      "grad_norm": 0.22169795632362366,
      "learning_rate": 9.760147601476015e-05,
      "loss": 0.7717,
      "step": 1130
    },
    {
      "epoch": 1.0412147505422993,
      "grad_norm": 0.24386726319789886,
      "learning_rate": 9.66789667896679e-05,
      "loss": 0.7841,
      "step": 1140
    },
    {
      "epoch": 1.0503482132663546,
      "grad_norm": 0.2329069972038269,
      "learning_rate": 9.575645756457565e-05,
      "loss": 0.7512,
      "step": 1150
    },
    {
      "epoch": 1.0594816759904098,
      "grad_norm": 0.25725606083869934,
      "learning_rate": 9.48339483394834e-05,
      "loss": 0.7877,
      "step": 1160
    },
    {
      "epoch": 1.0686151387144651,
      "grad_norm": 0.22388026118278503,
      "learning_rate": 9.391143911439116e-05,
      "loss": 0.7505,
      "step": 1170
    },
    {
      "epoch": 1.0777486014385205,
      "grad_norm": 0.23553399741649628,
      "learning_rate": 9.298892988929889e-05,
      "loss": 0.8001,
      "step": 1180
    },
    {
      "epoch": 1.0868820641625756,
      "grad_norm": 0.2409244328737259,
      "learning_rate": 9.206642066420664e-05,
      "loss": 0.7432,
      "step": 1190
    },
    {
      "epoch": 1.096015526886631,
      "grad_norm": 0.23572759330272675,
      "learning_rate": 9.11439114391144e-05,
      "loss": 0.7256,
      "step": 1200
    },
    {
      "epoch": 1.105148989610686,
      "grad_norm": 0.2237786501646042,
      "learning_rate": 9.022140221402214e-05,
      "loss": 0.7378,
      "step": 1210
    },
    {
      "epoch": 1.1142824523347414,
      "grad_norm": 0.24936777353286743,
      "learning_rate": 8.92988929889299e-05,
      "loss": 0.7992,
      "step": 1220
    },
    {
      "epoch": 1.1234159150587966,
      "grad_norm": 0.2569269835948944,
      "learning_rate": 8.837638376383764e-05,
      "loss": 0.7519,
      "step": 1230
    },
    {
      "epoch": 1.132549377782852,
      "grad_norm": 0.25572627782821655,
      "learning_rate": 8.74538745387454e-05,
      "loss": 0.7603,
      "step": 1240
    },
    {
      "epoch": 1.1416828405069073,
      "grad_norm": 0.24514737725257874,
      "learning_rate": 8.653136531365315e-05,
      "loss": 0.7963,
      "step": 1250
    },
    {
      "epoch": 1.1508163032309624,
      "grad_norm": 0.26390591263771057,
      "learning_rate": 8.560885608856088e-05,
      "loss": 0.7728,
      "step": 1260
    },
    {
      "epoch": 1.1599497659550178,
      "grad_norm": 0.27237701416015625,
      "learning_rate": 8.468634686346863e-05,
      "loss": 0.7464,
      "step": 1270
    },
    {
      "epoch": 1.1690832286790729,
      "grad_norm": 0.2341393530368805,
      "learning_rate": 8.376383763837639e-05,
      "loss": 0.7674,
      "step": 1280
    },
    {
      "epoch": 1.1782166914031282,
      "grad_norm": 0.25897255539894104,
      "learning_rate": 8.284132841328413e-05,
      "loss": 0.7751,
      "step": 1290
    },
    {
      "epoch": 1.1873501541271834,
      "grad_norm": 0.26896947622299194,
      "learning_rate": 8.191881918819189e-05,
      "loss": 0.7833,
      "step": 1300
    },
    {
      "epoch": 1.1964836168512387,
      "grad_norm": 0.2060190737247467,
      "learning_rate": 8.099630996309964e-05,
      "loss": 0.7293,
      "step": 1310
    },
    {
      "epoch": 1.205617079575294,
      "grad_norm": 0.24914538860321045,
      "learning_rate": 8.007380073800739e-05,
      "loss": 0.8091,
      "step": 1320
    },
    {
      "epoch": 1.2147505422993492,
      "grad_norm": 0.2311287522315979,
      "learning_rate": 7.915129151291514e-05,
      "loss": 0.7381,
      "step": 1330
    },
    {
      "epoch": 1.2238840050234046,
      "grad_norm": 0.2349044382572174,
      "learning_rate": 7.822878228782288e-05,
      "loss": 0.7904,
      "step": 1340
    },
    {
      "epoch": 1.2330174677474597,
      "grad_norm": 0.2494683861732483,
      "learning_rate": 7.730627306273062e-05,
      "loss": 0.8004,
      "step": 1350
    },
    {
      "epoch": 1.242150930471515,
      "grad_norm": 0.24623635411262512,
      "learning_rate": 7.638376383763838e-05,
      "loss": 0.7678,
      "step": 1360
    },
    {
      "epoch": 1.2512843931955704,
      "grad_norm": 0.24318312108516693,
      "learning_rate": 7.546125461254612e-05,
      "loss": 0.7896,
      "step": 1370
    },
    {
      "epoch": 1.2604178559196255,
      "grad_norm": 0.26625409722328186,
      "learning_rate": 7.453874538745388e-05,
      "loss": 0.7269,
      "step": 1380
    },
    {
      "epoch": 1.2695513186436806,
      "grad_norm": 0.2407088428735733,
      "learning_rate": 7.361623616236163e-05,
      "loss": 0.762,
      "step": 1390
    },
    {
      "epoch": 1.278684781367736,
      "grad_norm": 0.2567935287952423,
      "learning_rate": 7.269372693726938e-05,
      "loss": 0.7758,
      "step": 1400
    },
    {
      "epoch": 1.2878182440917914,
      "grad_norm": 0.2430599182844162,
      "learning_rate": 7.177121771217713e-05,
      "loss": 0.7784,
      "step": 1410
    },
    {
      "epoch": 1.2969517068158465,
      "grad_norm": 0.2551855444908142,
      "learning_rate": 7.084870848708487e-05,
      "loss": 0.8011,
      "step": 1420
    },
    {
      "epoch": 1.3060851695399018,
      "grad_norm": 0.2436087429523468,
      "learning_rate": 6.992619926199262e-05,
      "loss": 0.8173,
      "step": 1430
    },
    {
      "epoch": 1.315218632263957,
      "grad_norm": 0.2884344160556793,
      "learning_rate": 6.900369003690037e-05,
      "loss": 0.808,
      "step": 1440
    },
    {
      "epoch": 1.3243520949880123,
      "grad_norm": 0.24163366854190826,
      "learning_rate": 6.808118081180813e-05,
      "loss": 0.8607,
      "step": 1450
    },
    {
      "epoch": 1.3334855577120677,
      "grad_norm": 0.25318753719329834,
      "learning_rate": 6.715867158671587e-05,
      "loss": 0.7828,
      "step": 1460
    },
    {
      "epoch": 1.3426190204361228,
      "grad_norm": 0.25316768884658813,
      "learning_rate": 6.623616236162362e-05,
      "loss": 0.782,
      "step": 1470
    },
    {
      "epoch": 1.3517524831601782,
      "grad_norm": 0.26143646240234375,
      "learning_rate": 6.531365313653137e-05,
      "loss": 0.7747,
      "step": 1480
    },
    {
      "epoch": 1.3608859458842333,
      "grad_norm": 0.25864729285240173,
      "learning_rate": 6.439114391143912e-05,
      "loss": 0.7444,
      "step": 1490
    },
    {
      "epoch": 1.3700194086082886,
      "grad_norm": 0.29419568181037903,
      "learning_rate": 6.346863468634686e-05,
      "loss": 0.7813,
      "step": 1500
    },
    {
      "epoch": 1.379152871332344,
      "grad_norm": 0.2724389135837555,
      "learning_rate": 6.25461254612546e-05,
      "loss": 0.7429,
      "step": 1510
    },
    {
      "epoch": 1.3882863340563991,
      "grad_norm": 0.23915863037109375,
      "learning_rate": 6.162361623616236e-05,
      "loss": 0.7261,
      "step": 1520
    },
    {
      "epoch": 1.3974197967804545,
      "grad_norm": 0.2953547239303589,
      "learning_rate": 6.070110701107011e-05,
      "loss": 0.8155,
      "step": 1530
    },
    {
      "epoch": 1.4065532595045096,
      "grad_norm": 0.23125618696212769,
      "learning_rate": 5.9778597785977866e-05,
      "loss": 0.7794,
      "step": 1540
    },
    {
      "epoch": 1.415686722228565,
      "grad_norm": 0.2384129762649536,
      "learning_rate": 5.8856088560885615e-05,
      "loss": 0.7768,
      "step": 1550
    },
    {
      "epoch": 1.42482018495262,
      "grad_norm": 0.28964319825172424,
      "learning_rate": 5.7933579335793364e-05,
      "loss": 0.76,
      "step": 1560
    },
    {
      "epoch": 1.4339536476766754,
      "grad_norm": 0.28036704659461975,
      "learning_rate": 5.701107011070111e-05,
      "loss": 0.7473,
      "step": 1570
    },
    {
      "epoch": 1.4430871104007306,
      "grad_norm": 0.264535516500473,
      "learning_rate": 5.6088560885608855e-05,
      "loss": 0.7691,
      "step": 1580
    },
    {
      "epoch": 1.452220573124786,
      "grad_norm": 0.26369449496269226,
      "learning_rate": 5.5166051660516604e-05,
      "loss": 0.8043,
      "step": 1590
    },
    {
      "epoch": 1.4613540358488413,
      "grad_norm": 0.24130557477474213,
      "learning_rate": 5.424354243542435e-05,
      "loss": 0.722,
      "step": 1600
    },
    {
      "epoch": 1.4704874985728964,
      "grad_norm": 0.26355892419815063,
      "learning_rate": 5.332103321033211e-05,
      "loss": 0.7772,
      "step": 1610
    },
    {
      "epoch": 1.4796209612969518,
      "grad_norm": 0.2742040753364563,
      "learning_rate": 5.239852398523986e-05,
      "loss": 0.7706,
      "step": 1620
    },
    {
      "epoch": 1.4887544240210069,
      "grad_norm": 0.2509555220603943,
      "learning_rate": 5.1476014760147606e-05,
      "loss": 0.7562,
      "step": 1630
    },
    {
      "epoch": 1.4978878867450622,
      "grad_norm": 0.28320109844207764,
      "learning_rate": 5.0553505535055354e-05,
      "loss": 0.7806,
      "step": 1640
    },
    {
      "epoch": 1.5070213494691176,
      "grad_norm": 0.26480478048324585,
      "learning_rate": 4.96309963099631e-05,
      "loss": 0.7473,
      "step": 1650
    },
    {
      "epoch": 1.5161548121931727,
      "grad_norm": 0.24779197573661804,
      "learning_rate": 4.870848708487085e-05,
      "loss": 0.7445,
      "step": 1660
    },
    {
      "epoch": 1.5252882749172278,
      "grad_norm": 0.24852198362350464,
      "learning_rate": 4.77859778597786e-05,
      "loss": 0.774,
      "step": 1670
    },
    {
      "epoch": 1.5344217376412832,
      "grad_norm": 0.2635722756385803,
      "learning_rate": 4.686346863468635e-05,
      "loss": 0.7919,
      "step": 1680
    },
    {
      "epoch": 1.5435552003653386,
      "grad_norm": 0.2584611773490906,
      "learning_rate": 4.59409594095941e-05,
      "loss": 0.8048,
      "step": 1690
    },
    {
      "epoch": 1.552688663089394,
      "grad_norm": 0.250522643327713,
      "learning_rate": 4.501845018450185e-05,
      "loss": 0.7643,
      "step": 1700
    },
    {
      "epoch": 1.561822125813449,
      "grad_norm": 0.25920578837394714,
      "learning_rate": 4.4095940959409596e-05,
      "loss": 0.7727,
      "step": 1710
    },
    {
      "epoch": 1.5709555885375042,
      "grad_norm": 0.2565813660621643,
      "learning_rate": 4.3173431734317345e-05,
      "loss": 0.7549,
      "step": 1720
    },
    {
      "epoch": 1.5800890512615595,
      "grad_norm": 0.2492118626832962,
      "learning_rate": 4.2250922509225094e-05,
      "loss": 0.776,
      "step": 1730
    },
    {
      "epoch": 1.5892225139856149,
      "grad_norm": 0.25625908374786377,
      "learning_rate": 4.132841328413284e-05,
      "loss": 0.7275,
      "step": 1740
    },
    {
      "epoch": 1.59835597670967,
      "grad_norm": 0.25755420327186584,
      "learning_rate": 4.040590405904059e-05,
      "loss": 0.715,
      "step": 1750
    },
    {
      "epoch": 1.6074894394337254,
      "grad_norm": 0.28574299812316895,
      "learning_rate": 3.948339483394834e-05,
      "loss": 0.8222,
      "step": 1760
    },
    {
      "epoch": 1.6166229021577805,
      "grad_norm": 0.2426093965768814,
      "learning_rate": 3.856088560885609e-05,
      "loss": 0.7604,
      "step": 1770
    },
    {
      "epoch": 1.6257563648818358,
      "grad_norm": 0.26451465487480164,
      "learning_rate": 3.763837638376384e-05,
      "loss": 0.7667,
      "step": 1780
    },
    {
      "epoch": 1.6348898276058912,
      "grad_norm": 0.2830713093280792,
      "learning_rate": 3.6715867158671594e-05,
      "loss": 0.712,
      "step": 1790
    },
    {
      "epoch": 1.6440232903299463,
      "grad_norm": 0.28224441409111023,
      "learning_rate": 3.5793357933579336e-05,
      "loss": 0.7683,
      "step": 1800
    },
    {
      "epoch": 1.6531567530540014,
      "grad_norm": 0.28674688935279846,
      "learning_rate": 3.4870848708487085e-05,
      "loss": 0.7764,
      "step": 1810
    },
    {
      "epoch": 1.6622902157780568,
      "grad_norm": 0.3052882254123688,
      "learning_rate": 3.3948339483394833e-05,
      "loss": 0.7409,
      "step": 1820
    },
    {
      "epoch": 1.6714236785021122,
      "grad_norm": 0.23383349180221558,
      "learning_rate": 3.302583025830259e-05,
      "loss": 0.7489,
      "step": 1830
    },
    {
      "epoch": 1.6805571412261675,
      "grad_norm": 0.27901750802993774,
      "learning_rate": 3.210332103321033e-05,
      "loss": 0.7707,
      "step": 1840
    },
    {
      "epoch": 1.6896906039502226,
      "grad_norm": 0.2858339846134186,
      "learning_rate": 3.118081180811808e-05,
      "loss": 0.7414,
      "step": 1850
    },
    {
      "epoch": 1.6988240666742778,
      "grad_norm": 0.2798399329185486,
      "learning_rate": 3.0258302583025832e-05,
      "loss": 0.7778,
      "step": 1860
    },
    {
      "epoch": 1.7079575293983331,
      "grad_norm": 0.3009571433067322,
      "learning_rate": 2.9335793357933584e-05,
      "loss": 0.7733,
      "step": 1870
    },
    {
      "epoch": 1.7170909921223885,
      "grad_norm": 0.25638025999069214,
      "learning_rate": 2.8413284132841326e-05,
      "loss": 0.7198,
      "step": 1880
    },
    {
      "epoch": 1.7262244548464438,
      "grad_norm": 0.23823297023773193,
      "learning_rate": 2.749077490774908e-05,
      "loss": 0.7074,
      "step": 1890
    },
    {
      "epoch": 1.735357917570499,
      "grad_norm": 0.26458343863487244,
      "learning_rate": 2.6568265682656828e-05,
      "loss": 0.7871,
      "step": 1900
    },
    {
      "epoch": 1.744491380294554,
      "grad_norm": 0.2361028790473938,
      "learning_rate": 2.564575645756458e-05,
      "loss": 0.7098,
      "step": 1910
    },
    {
      "epoch": 1.7536248430186094,
      "grad_norm": 0.29253914952278137,
      "learning_rate": 2.472324723247233e-05,
      "loss": 0.7479,
      "step": 1920
    },
    {
      "epoch": 1.7627583057426648,
      "grad_norm": 0.26123401522636414,
      "learning_rate": 2.3800738007380074e-05,
      "loss": 0.756,
      "step": 1930
    },
    {
      "epoch": 1.77189176846672,
      "grad_norm": 0.29539722204208374,
      "learning_rate": 2.2878228782287826e-05,
      "loss": 0.7527,
      "step": 1940
    },
    {
      "epoch": 1.781025231190775,
      "grad_norm": 0.2475336194038391,
      "learning_rate": 2.195571955719557e-05,
      "loss": 0.812,
      "step": 1950
    },
    {
      "epoch": 1.7901586939148304,
      "grad_norm": 0.26271528005599976,
      "learning_rate": 2.1033210332103324e-05,
      "loss": 0.7823,
      "step": 1960
    },
    {
      "epoch": 1.7992921566388858,
      "grad_norm": 0.2526778280735016,
      "learning_rate": 2.011070110701107e-05,
      "loss": 0.7832,
      "step": 1970
    },
    {
      "epoch": 1.808425619362941,
      "grad_norm": 0.2669207751750946,
      "learning_rate": 1.918819188191882e-05,
      "loss": 0.7154,
      "step": 1980
    },
    {
      "epoch": 1.8175590820869962,
      "grad_norm": 0.3369690775871277,
      "learning_rate": 1.826568265682657e-05,
      "loss": 0.7651,
      "step": 1990
    },
    {
      "epoch": 1.8266925448110514,
      "grad_norm": 0.2714228332042694,
      "learning_rate": 1.734317343173432e-05,
      "loss": 0.7856,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 2188,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "total_flos": 4.658579268175135e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
